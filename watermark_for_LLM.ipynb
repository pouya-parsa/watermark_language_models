{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-V_7_ZpNQSX",
        "outputId": "a48a6a68-e0b7-41ba-af80-2593f2f4c40f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer"
      ],
      "metadata": {
        "id": "sSyruNrYNRjr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-large', model_max_length=1024)\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbkPeMAoNUYy",
        "outputId": "97e92bfd-a375-4999-9d6c-324db269b41d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This means that tokens that come after special tokens will not be properly handled. We recommend you to read the related pull request available at https://github.com/huggingface/transformers/pull/24565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Potential harms of large language models can be mitigated\n",
        "by watermarking model output, i.e., embedding signals into\n",
        "generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a\n",
        "watermarking framework for proprietary language models.\n",
        "The watermark can be embedded with negligible impact\n",
        "on text quality, and can be detected using an efficient opensource algorithm without access to the language model API\n",
        "or parameters. The watermark works by selecting a randomized set of “green” tokens before a word is generated, and\n",
        "then softly promoting use of green tokens during sampling.\"\"\"\n",
        "inputs = tokenizer([\"summarize: \" + text], return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "nqunfq5FNZKE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = model.generate(**inputs)\n",
        "print(tokenizer.decode(summary[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVlXGVvlNasZ",
        "outputId": "1cbf9305-e339-46bc-ad56-3a2a55d89c9d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad>watermarking language models can mitigate potential harms. watermarks can be embedded with\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jwkirchenbauer/lm-watermarking.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoJLv65hROY8",
        "outputId": "317b7391-f549-46b1-ac26-8b6853032533"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lm-watermarking'...\n",
            "remote: Enumerating objects: 313, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 313 (delta 10), reused 5 (delta 5), pack-reused 288\u001b[K\n",
            "Receiving objects: 100% (313/313), 11.98 MiB | 8.21 MiB/s, done.\n",
            "Resolving deltas: 100% (87/87), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/content/lm-watermarking\")"
      ],
      "metadata": {
        "id": "i2qUabqgRQQg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from watermark_processor import WatermarkDetector, WatermarkLogitsProcessor\n",
        "from transformers import (LogitsProcessorList)"
      ],
      "metadata": {
        "id": "4s0ADBx7Reoe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " input_text = (\n",
        "        \"The diamondback terrapin or simply terrapin (Malaclemys terrapin) is a \"\n",
        "        \"species of turtle native to the brackish coastal tidal marshes of the \"\n",
        "        \"Northeastern and southern United States, and in Bermuda.[6] It belongs \"\n",
        "        \"to the monotypic genus Malaclemys. It has one of the largest ranges of \"\n",
        "        \"all turtles in North America, stretching as far south as the Florida Keys \"\n",
        "        \"and as far north as Cape Cod.[7] The name 'terrapin' is derived from the \"\n",
        "        \"Algonquian word torope.[8] It applies to Malaclemys terrapin in both \"\n",
        "        \"British English and American English. The name originally was used by \"\n",
        "        \"early European settlers in North America to describe these brackish-water \"\n",
        "        \"turtles that inhabited neither freshwater habitats nor the sea. It retains \"\n",
        "        \"this primary meaning in American English.[8] In British English, however, \"\n",
        "        \"other semi-aquatic turtle species, such as the red-eared slider, might \"\n",
        "        \"also be called terrapins. The common name refers to the diamond pattern \"\n",
        "        \"on top of its shell (carapace), but the overall pattern and coloration \"\n",
        "        \"vary greatly. The shell is usually wider at the back than in the front, \"\n",
        "        \"and from above it appears wedge-shaped. The shell coloring can vary \"\n",
        "        \"from brown to grey, and its body color can be grey, brown, yellow, \"\n",
        "        \"or white. All have a unique pattern of wiggly, black markings or spots \"\n",
        "        \"on their body and head. The diamondback terrapin has large webbed \"\n",
        "        \"feet.[9] The species is\"\n",
        ")\n",
        "\n",
        "# output_text = \"watermarking language models can mitigate potential harms. watermarks can be embedded with\""
      ],
      "metadata": {
        "id": "eUhyh6NHSDvE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXNI9Ry9XFug",
        "outputId": "7b42c811-bdad-4e5f-d667-88f3952888cb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1373"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "watermark_processor = WatermarkLogitsProcessor(vocab=list(tokenizer.get_vocab().values()),\n",
        "                                               gamma=0.25,\n",
        "                                               delta=2.0,\n",
        "                                               seeding_scheme=\"simple_1\")\n",
        "\n",
        "tokenized_input = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "print(tokenized_input[\"input_ids\"].shape)\n",
        "# note that if the model is on cuda, then the input is on cuda\n",
        "# and thus the watermarking rng is cuda-based.\n",
        "# This is a different generator than the cpu-based rng in pytorch!\n",
        "\n",
        "output_tokens = model.generate(**tokenized_input,\n",
        "                               logits_processor=LogitsProcessorList([watermark_processor]))\n",
        "\n",
        "# if decoder only model, then we need to isolate the\n",
        "# newly generated tokens as only those are watermarked, the input/prompt is not\n",
        "# output_tokens = output_tokens[:,tokenized_input[\"input_ids\"].shape[-1]:]\n",
        "\n",
        "output_text = tokenizer.batch_decode(output_tokens, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHJQRptKSN7b",
        "outputId": "49fc761e-19e4-48ca-8119-8d1b2652bba3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 356])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZtgrmKWgWM3i",
        "outputId": "28e6dfa3-2c1e-4c05-c786-454219f7656e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'hello everybody my name is pouya'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = \"hello everybody my name is pouya\""
      ],
      "metadata": {
        "id": "IYtipxOhXp1-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "watermark_detector = WatermarkDetector(vocab=list(tokenizer.get_vocab().values()),\n",
        "                                        gamma=0.25, # should match original setting\n",
        "                                        seeding_scheme=\"simple_1\", # should match original setting\n",
        "                                        device=\"cpu\", # must match the original rng device type\n",
        "                                        tokenizer=tokenizer,\n",
        "                                        z_threshold=4.0,\n",
        "                                        normalizers=[],\n",
        "                                        ignore_repeated_bigrams=False)\n",
        "\n",
        "score_dict = watermark_detector.detect(output_text) # or any other text of interest to analyze"
      ],
      "metadata": {
        "id": "10kluq5yNkOY"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjYs1iuKRZ_S",
        "outputId": "55049853-c651-4b47-a24a-7185c02049b9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_tokens_scored': 7,\n",
              " 'num_green_tokens': 3,\n",
              " 'green_fraction': 0.42857142857142855,\n",
              " 'z_score': 1.091089451179962,\n",
              " 'p_value': 0.13761676203741713,\n",
              " 'prediction': False}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hbf8_btBXr5m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}